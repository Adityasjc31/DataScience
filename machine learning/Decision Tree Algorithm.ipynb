{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20/10/23 - Decision Tree\n",
    "A decision tree is a `non-parametric supervised learning` algorithm,which is utilized for both classification and regression tasks. It has a hierarchical,tree structure,which consists of a root node,branches,internal nodes and leaf nodes. <br>\n",
    " - `Non - parametric - which is not bounded by any assumptions` \n",
    " - It is also used for classification and regression\n",
    " - leaf node is decision node \n",
    " - data is fed to root node \n",
    " - Binary evaluation takes place \n",
    "`Pruning` - to overcome overfitting by ending at n-1 step of decision tree; this is a process which removes branches that split on features with low importance<br>\n",
    "Dividing until it get homogeneous dataset <br>\n",
    "However, as a tree grows in size, it becomes increasingly difficult to maintain this purity, and it usually results in too little data falling within a given subtree. When this occurs, it is known as data fragmentation and it can often lead to overfiting. <br>\n",
    "<hr>\n",
    "\n",
    "## Types of Decision Tree :\n",
    "- **Regular** : All the terminal nodes are pure. (No missing values)\n",
    "- **Irregular** : Some of the terminal nodes may have missing or non-numeric value.\n",
    "- **Gini Index** : Measures impurity in a set of instances, lower index indicates better split.\n",
    "- **Information Gain** : Measure of information gain from splitting on a particular attribute.\n",
    "- **ID3(Iterative Dichotomiser 3) Algorithm** : Selects best attribute based on Information Gain.This algorithm leverages entropy(measure of randomness) and information gain as metrics to evaluate candiate splits.\n",
    "    - entropy has inverse relation with information gain\n",
    "- **C4.5 algorithm** : Improved version of ID3 that selects attributes with equal frequency as well.\n",
    "- **CART(Classification & Regression Trees)** : Uses binary splits only. It typically uses gini impurity to split. Gini impurity measures how often a randomly chosen attribute is misclassified\n",
    "<hr>\n",
    "\n",
    "## How to Choose the best attribute at each node :\n",
    "- **First Split** : Select the attribute with maximum information gain.\n",
    "- **Second Split** : If there's no tie then select the attribute with minimum entropy.\n",
    "- **Third Split** : Use C4.5 algorithm.\n",
    "- **Gini impurity**\n",
    "<hr>\n",
    "\n",
    "`Entropy : It is a concept that stems from information theory, which measures the impurity of the sample values` <br>\n",
    "#### Formula : \n",
    " Entropy (S) = - Sigma(c belongs to C) p(c) $log_{2}$ p(c)\n",
    " - S - represnts the data set that entropy is calculated\n",
    " - c represents the classes in set S\n",
    " - p(c) represents the proportion of data points that belong to class c to the number of total data points in set S\n",
    " <hr>\n",
    "\n",
    " Range of S : 0 <= S <= 1<br>\n",
    " \n",
    " 0 means all samples in dataset S belong to one class<br>\n",
    " 1 means half of the samples are classified as one class and the other half are in another class<br>\n",
    " <hr>\n",
    "\n",
    "In order to select the best feature to split on and find the optimal decision tree, the attribute with the smallest amount of entropy should be used.\n",
    "<hr>\n",
    "\n",
    "Information gain represents the difference in entropy before and after a split on a given attribute.The attribute with the highest information gain will produce the best split as it's doing the best job at classifying the training data according to its target classification. \n",
    "<hr>\n",
    "\n",
    "**Information gain formula = 1 - entropy**<br>\n",
    "<hr>\n",
    "\n",
    "## Gini Impurity\n",
    "\n",
    "Gini impurity is the probability of incorrectly classifying random data point in the dataset if it were labeled based on the class distribution of the dataset. Similar entropy, if set S, is pure,i.e.,belonging to one class, then , its impurity is zero. This denoted by the formula :\n",
    " Gini Impurity = 1 - Sigma(i)($p_{i}$)^2\n",
    "<hr>\n",
    "\n",
    "## Advantages : \n",
    "- Easy to interupt\n",
    "- Little to no data preparation required\n",
    "- More flexible\n",
    "## Disadvantages :\n",
    "- Prone to overfitting\n",
    "    - Pre-pruning halts tree growth when there is insufficient data\n",
    "    - Post-pruning removes subtrees with inadequate data after tree construction\n",
    "- High variance estimators\n",
    "- More costly\n",
    "- Not fully supported in scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
